# 인공지능 위키

## 목차 (Table of Contents)

1. [개요](#1-개요)
2. [종류](#2-종류)
   1. [퍼셉트론](#21-퍼셉트론)
   2. [RNN](#22-rnn)
   3. [LSTM](#23-lstm)
   4. [GRU](#24-gru)
   5. [CNN](#25-cnn)
   6. [GAN](#26-gan)
   7. [Diffusion](#27-diffusion)
   8. [Transformer](#28-transformer)
3. [역전파](#3-역전파)
   1. [옵티마이저](#31-옵티마이저)
   2. [문제](#32-문제)
      1. [기울기 소실](#321-기울기-소실)
      2. [과적합](#322-과적합)
      3. [과소적합](#323-과소적합)
4. [LLM](#4-llm)
   1. [구조](#41-구조)
      1. [Attention](#411-attention)
      2. [FFNN](#412-ffnn)
      3. [위치 인코딩](#413-위치-인코딩)
   2. [최적화](#42-최적화)
      1. [Speculative Decoding](#421-speculative-decoding)
      2. [KV 캐싱](#422-kv-캐싱)
      3. [Chunked Prefill](#423-chunked-prefill)
      4. [Weight tying](#424-weight-tying)
   3. [문제](#43-문제)
      1. [파멸적 망각](#431-파멸적-망각)
5. [활성화 함수](#5-활성화-함수)
   1. [시그모이드](#51-시그모이드)
   2. [하이퍼볼릭 탄젠트](#52-하이퍼볼릭-탄젠트)
   3. [ReLU](#53-relu)
      1. [Leaky ReLU](#531-leaky-relu)
      2. [GELU](#532-gelu)
6. [역사](#6-역사)
7. [여담](#7-여담)
   1. [인공지능의 미래 전망](#71-인공지능의-미래-전망)
   2. [인공지능의 대표 활용 분야](#72-인공지능의-대표-활용-분야)

## 1. 개요

인공지능(Artificial Intelligence)은 기계가 인간처럼 학습하고 판단하며 문제를 해결할 수 있도록 만드는 기술을 의미한다.  
단순히 명령을 수행하는 수준을 넘어, 스스로 데이터를 분석하고 패턴을 찾으며 새로운 결과를 생성할 수 있는 능력을 갖추는 것이 핵심이다.  
초기에는 규칙 기반 시스템으로 시작하였으나, 현재에는 더 정교화된 기술을 중심으로 빠르게 발전하고 있으며, 다양한 알고리즘과 학습 기법을 통해 데이터를 분석하고 스스로 성능을 향상시킨다.  
인공지능의 종류에는 기계 학습, 딥러닝, 전문가 시스템 등 여러 방식이 존재하며, 특히 역전파(BackPropagation)와 활성화 함수는 딥러닝 모델의 성능을 높이는 핵심 구조로 사용된다. 최근에는 LLM(대규모 언어 모델)의 등장으로 AI가 단순 인식에서 벗어나 텍스트 이미지 코드 등 창의적 콘텐츠 생성까지 수행할 수 있게 되었고, 이처럼 기술이 발전해 온 과정은 인공지능의 역사 전반을 통해 확인할 수 있다. 인공지능은 산업 경제 일상생활 전반에서 활용되며 미래 핵심 기술로 자리 잡고 있다.

## 2. 종류

### 2.1. 퍼셉트론

1957년 프랑크 로젠블라트가 개발한 인간의 뉴런을 모방해서 만든 수학적 모델이다. 최초의 인공지능 모델이라고 할 수 있으며, 현재 사용하는 모든 인공지능의 기초가 된다. 수학식은 다음과 같다.

$$
y = f(\sum_i{w_ix_i+b})
$$

SLP(Single Layer Perceptron)로는 XOR 문제를 해결하지 못하였고, 이 때문에 전문가 시스템이나 SVM(Support Vector Machine) 등이 인공지능의 주류였으나, MLP(Multi Layer Perceptron)은 XOR 문제를 해결할 수 있고, GPU 등의 하드웨어와 역전파 등의 알고리즘이 추가적으로 나타나며 주류가 되었다.

### 2.2. RNN

자연어 처리를 위해 개발된 모델로, 신경망을 재활용한다. 순차적으로 입력을 읽기 때문에 시퀀스가 길수록 처리 속도가 길어진다. 입력을 받으면 은닉 정보와 출력을 생성하며, 은닉 정보는 다음 입력과 함께 계산된다. RNN의 문제는 장기 의존성인데, 초기 정보가 순차적으로 은닉 정보에 축적되어 전달되는 형태이기 때문에, 이전 정보를 점차 잊게 된다. 시퀀스가 짧은 경우 상관 없지만, 문맥을 신경 써야 되는 경우 좋은 성능을 보지 못한다. 이를 해결하기 위해 LSTM, GRU가 등장하였다.

### 2.3. LSTM

RNN의 장기 의존성 문제를 해결하기 위해 여러 gate를 추가한 모델이다. LSTM은 망각 게이트, 입력 게이트, 출력 게이트가 존재한다. 어떤 정보를 잊고, 어떤 정보를 유지할지 결정하며 RNN에 비해 더 긴 시퀀스를 유지할 수 있으나 여전히 순차적으로 처리하기 때문에 시퀀스의 길이에 비례한 느린 속도를 지니고 있다.

### 2.4. GRU

기존 LSTM을 간소화한 버전으로, 성능은 LSTM과 비슷하거나 낮지만 파라미터가 적어 효율적이다. LSTM의 경우 망각, 입력, 출력 3가지의 게이트가 있고 cell과 hidden state로 2개가 있지만, GRU의 경우 리셋, 업데이트 게이트 2개에 hidden state 하나만 가지고 있다.

### 2.5. CNN

입력 전체를 보는 것이 아니라, 특정한 window로 특징을 집어내는 구조. 주로 이미지 처리에 사용되며, dense, fc 또는 linear 레이어 비해 낮은 파라미터를 가지고 있다.

### 2.6. GAN

생성적 적대 신경망(Generative Adversarial Network)은 구조나 모델이라기 보다 학습 방법의 일종으로, 주로 이미지 처리에 사용된다. 생성 모델과 구별 모델 두 가지 모델로 학습되며, 생성 모델의 목적은 원본과 구분되지 않는 것을 생성하는 것이며, 구별 모델은 원본과 생성 모델이 생성한 가짜를 구별하는 것을 목적으로 한다. 학습 데이터가 적어도 학습된다는 장점이 있지만 두 모델을 모두 학습시켜야 한다는 부담이 존재한다.

### 2.7. Diffusion

확산(Diffusion) 모델은 주로 이미지 생성에 사용되는 강력한 모델이다. 이미지를 점차 노이즈로 가득하게 만들고, 그걸 다시 깔끔한 이미지로 만드는 과정을 반복하며 학습한다. 굉장히 높은 수준의 생성 능력을 보이지만, 노이즈를 거두는 과정을 몇 번이나 반복해야 해서 생성 속도가 매우 느리다. 오디오도 이미지의 일종으로 볼 수 있어 오디오를 포함해서 비디오까지 생성 가능하다.

### 2.8. Transformer

ChatGPT, Gemini, Claude, LLaMA, Grok 등 거의 모든 LLM이 차용하고 있는 구조이다. 일반적으로 Encoder-Decdoer 구조이지만, LLM의 경우 Decoder only 구조를 주로 사용한다. Attention을 통해 장기 의존성과 병렬 처리를 통한 느린 속도 문제를 해결하였다. DiT, ViT 등 이미지 생성을 위해 사용되기도 하며, Cross-Attention 등을 사용해 이미지 등을 토큰화하여 멀티 모달로도 사용할 수 있다.

## 3. 역전파

역전파(BackPropagation)는 인공지능을 학습하기 위한 방법이다. 특히 MLP과 같이 깊은 레이어를 학습시키기 위해 등장하였다. 역전파로 구하는 가중치는 이번 입력에만 해당되는 가중치이므로, 그냥 적용하면 가중치가 극단적으로 변화하여 발산하거나 일반성을 잃어 성능이 좋지 않을 수 있다. 이 때문에 학습률을 적용하여 적절한 크기로 가중치를 업데이트한다. 학습률이 클 경우 학습 자체는 빠르나 과적합이 생기거나 발산할 수 있고, 학습률이 작을 경우 학습이 되지 않거나 학습이 지나치게 느릴 수 있어 적절한 학습률을 택해야 한다.

### 3.1. 옵티마이저

![옵티마이저 발달 계보](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTejbXOeuW7VTnCsAIkC01U-DnpbZCqy2d9Ig&s)

보통 역전파를 비유할 때 눈을 감고 하산하는 것에 비유한다. 이때 어느 방향으로 어떤 속도로 발걸음을 옮길지를 결정하는 것이 옵티마이저(Optimizer)이다. 일반적으로 Adam 또는 AdamW가 가장 많이 사용된다.

### 3.2. 문제

#### 3.2.1. 기울기 소실

기울기 소실(Gradient Vanishing)은 레이어가 깊어지면 학습이 되지 않는 문제이다. 더 정확히는 어떻게 가중치를 업데이트해야 되는지에 대한 기울기가 사라져서 업데이트되지 않는 것을 말한다. 이 문제는 ReLU와 같은 활성화 함수를 사용하거나, 이전 레이어의 정보와 합치는 방식 등을 이용해 해결할 수 있다.

#### 3.2.2. 과적합

과적합(Overfitting)은 모델이 훈련 데이터에는 완벽한 성능을 보이지만, 일반성은 떨어져서 테스트에서는 낮은 성능을 보이게 되는 현상을 말한다. 쉽게 말하면 문제를 이해한 게 아니라 외워서 풀어서 동일한 유형의 새로운 문제가 오면 풀지 못하는 것과 같은 상태를 의미한다. 학습률을 너무 크게 하거나, epoch이 높거나 데이터가 적거나, 데이터에 비해 모델의 파라미터가 너무 클 때 발생할 수 있다. L2 정규화, dropout, 학습률을 내리는 것과 같은 방식으로 해결할 수 있다.

#### 3.2.3. 과소적합

과소적합(Underfitting)은 과적합과는 달리 아예 학습이 되지 않은 상태를 의미한다. 이 경우 학습 데이터가 너무 적거나, 모델 파라미터가 충분하지 않거나, epoch이 적어서일 수 있다. 혹은 과적합을 방지하기 위해 dropout, L2 정규화 등을 너무 강하게 했을 경우에도 발생할 수 있다. 학습률을 키우거나 파라미터를 키우거나, 또는 데이터를 늘리는 등의 방법으로 해결할 수 있다.

## 4. LLM

### 4.1. 구조

![Default Transformer Image](https://i.namu.wiki/i/a0XOwuo-7QA7UvqZuxJBiGE2jtzKQPTo-Rzs2PZjqCAYL8gtdyoYKyuAGgVSKB32N61Uxvh_ahUWvin5jrGoLu4fDWpRGxSmReNQcAS7v3vByD2CbnNOG2bEARf-ncNySdT4rPbaY2GPKXHt-HquGw.webp)

현재 서비스 중인 LLM의 대부분은 Transformer 구조를 사용하고 있다. 특히 Decoder only 구조를 차용하고 있기에 이 문서에서는 Decoder only Transformer에 대해 설명한다.

#### 4.1.1. Attention

다른 토큰과의 관계를 파악하는 계산이다. 계산 복잡도가 $O(n^2)$으로 매우 크나, Transformer의 핵심이 되는 부분이다. Attention의 수학적 식은 다음과 같다.

$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

이해하기 쉽게 설명하면 어떤 검색어(Q)를 입력하면 그 쿼리에 맞는 사이트(K)가 나타나고, 검색어에 맞는 사이트(K)라면 들어가서 그 내용(V)을 보는 느낌이라고 할 수 있다. 더 정확히는 Query와 Key간의 유사도를 계산해서 유사도 대로 Value를 갖게 된다.

높은 계산 복잡도로 인해 이를 해결하기 위한 변형이 많다. 예를 들어 MQA의 여러 개의 Q에 하나의 KV를 가지고 계산한다. 다만 성능이 매우 떨어진다. 그래서 나온 것이 GQA로, 그룹화된 KV를 가진다. 이 경우 Q와 동일한 개수의 KV를 가지는 일반적인 MHA에 비해 효율적이고 성능 또한 동일하거나 높다. MLA의 경우 KV를 압축하는 네트워크를 만들어서 계산하는 것이 있다. 이 경우 KV 캐싱이 효율적인데다가 성능 또한 MHA와 GQA에 비해 높다는 평가가 있다. 이외에는 Sliding Window Attention, Performer Attention (Softmax에 근사하는 방법) 등이 있다.

#### 4.1.2. FFNN

일반적인 뉴런 네트워크이다. PyTorch로 구현할 경우 다음과 같은 코드로 구현할 수 있다.

```python
import torch.nn as nn

dim = 128
ffnn_dim = 512

layer = nn.Sequential(
    nn.Linear(dim, ffnn_dim),
    nn.GELU(),
    nn.Linear(ffnn_dim, dim)
)
```

ffnn_dim은 dim의 4배로 잡는 편이다. 또한, 활성화 함수도 ReLU보다는 GELU를 사용하나, 다른 함수를 사용해도 된다. 최신 오픈 소스 모델들을 보면 이러한 형태보다는 MoE나 xxxGLU의 형태를 띈다. xxxGLU의 경우 다음과 같은 코드로 구현해볼 수 있다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

MoE의 경우 모델 전체가 아닌 일부만 사용하는 방법으로 파라미터 효율성을 증가시킬 수 있다. router와 topk를 통해 어떤 전문가를 선택할지 결정해서 그 전문가로만 계산한다. 그러나 전문가가 많을 경우 특정 전문가만 선택되어 학습이 고르지 못하게 될 수도 있다. 이 경우 전문가를 골고루 선택할 수 있게 loss를 추가하거나, 동적 편향을 추가할 수 있다. 동적 편향의 경우 전문가 선택 통계를 내어 선택되지 못한 전문가의 편향은 높게, 많이 선택된 전문가는 낮게 주어 고르게 학습되게 할 수 있다. 공유 전문가를 도입할 수도 있는데, 항상 활성화되는 전문가이다. 공통된 정보는 공유 전문가를 통해 처리하여 각 전문가가 더 '전문적'이게 할 수 있다.

### 4.1.3. 위치 인코딩

위치 인코딩(Positional Encoding)은 Transformer가 입력을 병렬 처리할 수 있는 이유이다. 입력에 위치를 알리는 벡터를 추가해 모델이 토큰의 위치를 알 수 있게 한다. 보통 사인(sine)과 코사인(cosine) 함수를 사용한다.

$$
PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})
$$

이때 $d_{model}$은 임베딩 차원을 의미한다. 충분한 차원이 확보되어 있다면 주기 함수이기 때문에 중복되지 않는 값을 가지면서도 무작위 노이즈가 아닌 예측 가능한 정보가 된다.

### 4.2. 최적화

#### 4.2.1. Speculative Decoding

대형 언어 모델은 강력하지만 autoregressive 모델 특성상 순차적으로 토큰을 생성하며 생성 속도가 비교적 느릴 수밖에 없다. 그러나 하나씩 생성하는 것과 다르게 읽기에 한정해서는 병렬 처리가 가능하다. 바로 이점에서 착안하여 비교적 작고 빠른 모델이 생성한 출력을 검증하는 방식으로 생성 속도를 높이는 것이 Speculative Decoding이다.

#### 4.2.2. KV 캐싱

Attention은 $O(n^2)$이라는 큰 계산 복잡도를 가진다. 특히 토큰 하나씩 생성하는 autoregressive 모델 특성상 이 계산을 반복한다면 매우 비효율적인 계산을 하게 된다. 따라서 이미 계산한 KV에 대해서는 저장하여 사용함으로서 계산 복잡도를 $O(n^2)$에서 $O(n)$으로 낮춘다. 특히 시스템 프롬프트와 같이 변하지 않는 데이터의 경우 미리 KV 캐싱을 실행하여 모델을 서빙할 때 효율적으로 처리할 수 있다.

#### 4.2.3. Chunked Prefill

앞서 언급되었듯 Attention은 $O(n^2)$의 계산 복잡도를 가진다. 이 말은 아무리 KV 캐싱을 한다고 해도 한 번에 매우 긴 시퀀스를 처리한다면 메모리적으로 큰 부담이 된다는 뜻이다. 이를 해결하기 위해 전체를 넣기 보다는 청크를 나눠서 계산하기도 한다. 즉, 입력 시퀀스가 100이라고 할 때 이걸 20개씩 5개로 나눠서 $O(20^2) \times 5$으로 계산하겠다는 것이다. 이 경우 $100^2 = 10000$인 원래 계산보다 $20^2 = 400, 400 \times 5 = 2000$으로 상당히 낮아짐을 확인할 수 있다. 물론, 실제 계산에서는 조금 더 복잡하게 작동하므로 위 계산에서처럼 5배나 효율적이게 되지는 않겠지만, 부담을 줄이기에는 충분한 방법이다.

#### 4.2.4. Weight tying

임베딩과 언임베딩을 공통된 레이어로 처리하는 것을 말한다. 이 경우 파라미터를 아낄 수 있고, 임베딩과 언임베딩이 공통된 공간을 사용해서 일관성을 높일 수 있다.

### 4.3. 문제

#### 4.3.1. 파멸적 망각

파멸적 망각(Catastrophic Forgetting)은 인공지능이 새로운 정보를 학습할 때 이전의 정보를 급격하게 상실하는 것을 말한다. 즉, 이전 정보를 유지한 채로 새로운 정보를 학습하는 게 아니라 이전 정보를 잊고 새로운 정보만을 기억하게 되는 것이다.

## 5. 활성화 함수

### 5.1. 시그모이드

시그모이드(Sigmoid)는 가장 대표적인 활성화 함수이다. 식은 다음과 같다.

$$
f(x) = \frac{e^x}{e^x+1}
$$

입력값을 0에서 1 사이의 값으로 바꾼다. 로지스틱 문제, 이진 분류 등의 문제에서 자주 사용된다.

### 5.2. 하이퍼볼릭 탄젠트

입력값을 -1에서 1 사이의 값으로 반환하는 함수이다. 식은 다음과 같다.

$$
f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

### 5.3. ReLU

입력값을 양수라면 양수 그대로, 음수 또는 0이라면 0을 반환하는 함수이다. 계산이 빠르고 기울기 소실 문제를 완화할 수 있어 여러 모델에서 자주 사용되는 활성화 함수이다.

$$
f(x) = 
    \begin{cases}
    x \quad (x > 0) \\
    0 \quad (x \leq 0)
    \end{cases}
$$

0 또는 음수라면 0을 반환하기 때문에, 음수 부분은 항상 0이라 학습이 되지 않는 죽은 뉴런 문제가 일어날 수 있다. 이를 보완하기 위한 여러 ReLU의 변형이 생겨났다.

#### 5.3.1. Leaky ReLU

Leaky ReLU는 죽은 뉴런 현상을 해결하기 위해 고안된 함수로, 음수일 때 0이 아니라 아주 작은 값을 반환한다. 식은 다음과 같다.

$$
f(x) = 
    \begin{cases}
    x \quad (x > 0) \\
    ax \quad (x \leq 0)
    \end{cases}
$$

이때 a는 작은 상수이다. (예를 들어 0.01과 같은)

#### 5.3.2. GELU

ReLU의 변형이다. 식은 다음과 같다.

$$
f(x) = \frac{1}{2}x\bigg(1 + erf\Big(\frac{x}{\sqrt{2}}\Big)\bigg)
$$

## 6. 역사

* 1940~50: AI 아이디어 탄생(튜링 테스트)
* 1950~70: AI의 시작, 규칙 기반, 퍼셉트론
* 1970~90: AI 겨울 -> 제한된 성능
* 1990~2010: 머신러닝 시대
* 2010~2020: 딥러닝 혁명
* 2020~현재: 생성형 AI시대 (ChatGPT)등

## 7. 여담

### 7.1. 인공지능의 미래 전망

미래의 인공지능은 인간의 지능을 보조하는 단계를 넘어 스스로 학습 추론 협업하는 지능형 파트너로 진화하며, 기술 발전과 윤리 정책의 균형이 핵심 과제로 자리 잡을 것이다.

### 7.2. 인공지능의 대표 활용 분야

* 자율 주행 자동차
* 이미지 얼굴 인식
* 의료 진단 시스템
* 챗봇 & ChatGPT등
